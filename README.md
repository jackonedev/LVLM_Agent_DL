# Multimodal RAG: Chat with Videos

### Part 1: Multimodal Embeddings

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/136rTHsohoFjTX_Lq6INhKHJEALpGz850/view?usp=sharing)

Let's imagine an (image-text) pair. This multimodal data pair can be processed by a multimodal embedding model. The model used during the lesson is called BridgeTower, and it generates an embedding of the image-text data pair into a 512-dimensional vector within a multimodal semantic space. In the notebook presented here, the CLIP ViT-B/32 model is used.

"In this way, a multimodal semantic space is obtained, where the multimodal data pair enhances the closeness of similar vectors in their vector representations, providing an additional level of representational power."
